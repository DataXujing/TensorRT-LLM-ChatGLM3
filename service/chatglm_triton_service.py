#!/usr/bin/env python
# -*- coding:utf-8 _*-

"""
ChatGLM service

我可以换成一个云服务的LLM
"""

import os
import sys
from functools import partial

sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))

import argparse
import queue
import numpy as np
import tritonclient.grpc as grpcclient
from tritonclient.utils import InferenceServerException, np_to_triton_dtype

import logging
from typing import Any, List, Mapping, Optional
from langchain.llms.base import LLM

from langchain.llms.utils import enforce_stop_tokens

logger = logging.getLogger(__name__)

client = None

class ChatGLMService(LLM):

    endpoint_url: str = "localhost:8001"
    model_kwargs: Optional[dict] = None
    max_token: int = 1024
    temperature: float = 0.1
    history: List[List] = []
    top_p: float = 0.9
    with_history: bool = False
    
    # client: grpcclient.InferenceServerClient = None
    
    @property
    def _llm_type(self) -> str:
        return "chat_glm3"

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """Get the identifying parameters."""
        _model_kwargs = self.model_kwargs or {}
        return {
            **{"endpoint_url": self.endpoint_url},
            **{"model_kwargs": _model_kwargs},
        }

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> str:
        """Call out to a ChatGLM LLM inference endpoint.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.

        Returns:
            The string generated by the model.

        Example:
            .. code-block:: python

                response = chatglm_llm("Who are you?")
        """
        
        # output_text = run_inference(
        #     self.client, prompt, FLAGS.output_len, FLAGS.request_id,
        #     FLAGS.repetition_penalty, FLAGS.presence_penalty,
        #     FLAGS.frequency_penalty, FLAGS.temperature, FLAGS.stop_words,
        #     FLAGS.bad_words, embedding_bias_words, embedding_bias_weights,
        #     FLAGS.model_name, FLAGS.streaming, FLAGS.beam_width,
        #     FLAGS.overwrite_output_text, return_context_logits_data,
        #     return_generation_logits_data, FLAGS.end_id, FLAGS.pad_id, True)
        output_text = run_inference(
            client, prompt, 1024, "",
            None, None,
            None, self.temperature, [],
            [], None, None,
            "tensorrt_llm_bls", False, 1,
            False, None,
            None, None, None, True)
        # output_text = prompt


        if stop is not None:
            output_text = enforce_stop_tokens(output_text, stop)
        if self.with_history:
            self.history = self.history + [[prompt, output_text]]
        return output_text
    
    def load_model(self):
        try:
            global client 
            client = grpcclient.InferenceServerClient(url=self.endpoint_url)
        except Exception as e:
            print("-------------->>>")
            print(str(e))
            sys.exit(1)
        

def prepare_tensor(name, input):
    t = grpcclient.InferInput(name, input.shape,
                              np_to_triton_dtype(input.dtype))
    t.set_data_from_numpy(input)
    return t

class UserData:
    def __init__(self):
        self._completed_requests = queue.Queue()


def callback(user_data, result, error):
    if error:
        user_data._completed_requests.put(error)
    else:
        user_data._completed_requests.put(result)
        

def run_inference(triton_client, prompt, output_len, request_id,
                  repetition_penalty, presence_penalty, frequency_penalty,
                  temperature, stop_words, bad_words, embedding_bias_words,
                  embedding_bias_weights, model_name, streaming, beam_width,
                  overwrite_output_text, return_context_logits_data,
                  return_generation_logits_data, end_id, pad_id, verbose):

    input0 = [[prompt]]
    input0_data = np.array(input0).astype(object)
    output0_len = np.ones_like(input0).astype(np.int32) * output_len
    streaming_data = np.array([[streaming]], dtype=bool)
    beam_width_data = np.array([[beam_width]], dtype=np.int32)
    temperature_data = np.array([[temperature]], dtype=np.float32)

    # 准备推理的input---------------------->>>
    inputs = [
        prepare_tensor("text_input", input0_data),
        prepare_tensor("max_tokens", output0_len),
        prepare_tensor("stream", streaming_data),
        prepare_tensor("beam_width", beam_width_data),
        prepare_tensor("temperature", temperature_data),
    ]

    if bad_words:
        bad_words_list = np.array([bad_words], dtype=object)
        inputs += [prepare_tensor("bad_words", bad_words_list)]

    if stop_words:
        stop_words_list = np.array([stop_words], dtype=object)
        inputs += [prepare_tensor("stop_words", stop_words_list)]

    if repetition_penalty is not None:
        repetition_penalty = [[repetition_penalty]]
        repetition_penalty_data = np.array(repetition_penalty,
                                           dtype=np.float32)
        inputs += [
            prepare_tensor("repetition_penalty", repetition_penalty_data)
        ]

    if presence_penalty is not None:
        presence_penalty = [[presence_penalty]]
        presence_penalty_data = np.array(presence_penalty, dtype=np.float32)
        inputs += [prepare_tensor("presence_penalty", presence_penalty_data)]

    if frequency_penalty is not None:
        frequency_penalty = [[frequency_penalty]]
        frequency_penalty_data = np.array(frequency_penalty, dtype=np.float32)
        inputs += [prepare_tensor("frequency_penalty", frequency_penalty_data)]

    if return_context_logits_data is not None:
        inputs += [
            prepare_tensor("return_context_logits",
                           return_context_logits_data),
        ]

    if return_generation_logits_data is not None:
        inputs += [
            prepare_tensor("return_generation_logits",
                           return_generation_logits_data),
        ]

    if (embedding_bias_words is not None and embedding_bias_weights is None
        ) or (embedding_bias_words is None
              and embedding_bias_weights is not None):
        assert 0, "Both embedding bias words and weights must be specified"

    if (embedding_bias_words is not None
            and embedding_bias_weights is not None):
        assert len(embedding_bias_words) == len(
            embedding_bias_weights
        ), "Embedding bias weights and words must have same length"
        embedding_bias_words_data = np.array([embedding_bias_words],
                                             dtype=object)
        embedding_bias_weights_data = np.array([embedding_bias_weights],
                                               dtype=np.float32)
        inputs.append(
            prepare_tensor("embedding_bias_words", embedding_bias_words_data))
        inputs.append(
            prepare_tensor("embedding_bias_weights",
                           embedding_bias_weights_data))
    if end_id is not None:
        end_id_data = np.array([[end_id]], dtype=np.int32)
        inputs += [prepare_tensor("end_id", end_id_data)]

    if pad_id is not None:
        pad_id_data = np.array([[pad_id]], dtype=np.int32)
        inputs += [prepare_tensor("pad_id", pad_id_data)]
    #---------------------------------------------------->>>
    
    user_data = UserData()  # queue进行异步推断
    # Establish stream
    triton_client.start_stream(callback=partial(callback, user_data))
    # Send request
    triton_client.async_stream_infer(model_name, inputs, request_id=request_id)

    #Wait for server to close the stream
    triton_client.stop_stream()
    # #改同步推断,decopuled 不行
    ## https://github.com/triton-inference-server/server/issues/4994
    # result = triton_client.infer(model_name, inputs, request_id=request_id)
    
    # Parse the responses
    output_text = ""
    while True:
        try:
            result = user_data._completed_requests.get(block=False)  #queue
        except Exception as e:
            break

        if type(result) == InferenceServerException:
            print("Received an error from server:")
            print(result)
        else:
            output = result.as_numpy('text_output')  # 解析返回的"text_output"的结点的值
            if streaming and beam_width == 1:
                new_output = output[0].decode("utf8")
                if overwrite_output_text:
                    output_text = new_output
                else:
                    output_text += new_output
            else:
                output_text = output[0].decode("utf8")
                if verbose:
                    print(output_text, flush=True)

            if return_context_logits_data is not None:
                context_logits = result.as_numpy('context_logits')
                print(f"context_logits.shape: {context_logits.shape}")
                print(f"context_logits: {context_logits}")
            if return_generation_logits_data is not None:
                generation_logits = result.as_numpy('generation_logits')
                print(f"generation_logits.shape: {generation_logits.shape}")
                print(f"generation_logits: {generation_logits}")

    if streaming and beam_width == 1:
        if verbose:
            print(output_text)

    return output_text
